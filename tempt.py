# xxxxxx
# # test_meta_feature_extraction.py

# import torch
# import numpy as np
# import pandas as pd
# import numpy as np
# import pandas as pd
# from scipy.stats import skew, kurtosis, entropy
# from scipy.signal import periodogram
# from statsmodels.tsa.stattools import acf, adfuller
# from statsmodels.tsa.seasonal import seasonal_decompose
# from statsmodels.tsa.ar_model import AutoReg


# def extract_meta_feature(data):
#     """
#     Extracts meta-features from a given time series data.

#     Parameters:
#     - data: np.ndarray, shape (n_samples, n_features), time series data

#     Returns:
#     - features: dict, contains the extracted meta-features
#     """
#     features = {}

#     # basic statistics
#     # axis=0 è¡¨ç¤ºæ²¿ç€æ—¶é—´è½´è®¡ç®—ï¼ˆå³å¯¹æ¯ä¸ªå˜é‡å•ç‹¬è®¡ç®—ï¼‰
#     # .mean() å†åœ¨æ‰€æœ‰å˜é‡ä¸Šå–å¹³å‡ â†’ å¾—åˆ°ä¸€ä¸ªç»¼åˆæ€§çš„æ ‡é‡ç‰¹å¾
#     features["mean"] = np.mean(data, axis=0).mean()
#     features["std"] = np.std(data, axis=0).mean()
#     features["min"] = np.min(data, axis=0).mean()
#     features["max"] = np.max(data, axis=0).mean()
#     features["skewness"] = np.nanmean(skew(data, axis=0))
#     features["kurtosis"] = np.nanmean(kurtosis(data, axis=0))

#     # time series decomposition
#     acfs = [acf(data[:, i], nlags=10, fft=True) for i in range(data.shape[1])]
#     features["autocorrelation_mean"] = np.nanmean(
#         [acf_val[1] for acf_val in acfs]
#     )  # first lag
#     adf_results = [adfuller(data[:, i]) for i in range(data.shape[1])]
#     features["stationarity"] = np.mean([result[1] < 0.05 for result in adf_results])

#     # rate_of_change = np.diff(data, axis=0) / data[:-1]
#     # Deal with 0 division
#     safe_data = np.where(data[:-1] == 0, np.nan, data[:-1])
#     rate_of_change = np.diff(data, axis=0) / safe_data
#     features["rate_of_change_mean"] = np.nanmean(rate_of_change)
#     features["rate_of_change_std"] = np.nanstd(rate_of_change)

#     # Landmarker features
#     autoreg_coefs, residual_stds = [], []
#     for i in range(data.shape[1]):
#         model = AutoReg(data[:, i], lags=1).fit()
#         autoreg_coefs.append(model.params[1])
#         residual_stds.append(np.std(model.resid))
#     features["autoreg_coef_mean"] = np.mean(autoreg_coefs)
#     features["residual_std_mean"] = np.mean(residual_stds)

#     # frequency domain features
#     freq_means, freq_peaks, spectral_entropies = [], [], []
#     spectral_variations, spectral_skewnesses, spectral_kurtoses = [], [], []

#     for i in range(data.shape[1]):
#         freqs, psd = periodogram(data[:, i])
#         freq_means.append(np.mean(psd))
#         freq_peaks.append(freqs[np.argmax(psd)])
#         spectral_entropies.append(entropy(psd))
#         if i > 0:
#             prev_psd = periodogram(data[:, i - 1])[1]
#             spectral_variations.append(np.sqrt(np.sum((psd - prev_psd) ** 2)))
#         else:
#             spectral_variations.append(0)  # ç¬¬ä¸€ä¸ªå˜é‡æ— æ³•è®¡ç®—å˜åŒ–
#         spectral_skewnesses.append(skew(psd))
#         spectral_kurtoses.append(kurtosis(psd))

#     features["frequency_mean"] = np.mean(freq_means)
#     features["frequency_peak"] = np.mean(freq_peaks)
#     features["spectral_entropy"] = np.nanmean(spectral_entropies)
#     features["spectral_variation"] = np.nanmean(spectral_variations)
#     features["spectral_skewness"] = np.nanmean(spectral_skewnesses)
#     features["spectral_kurtosis"] = np.nanmean(spectral_kurtoses)

#     cov_matrix = np.cov(data, rowvar=False)
#     features["covariance_mean"] = np.mean(cov_matrix)
#     features["covariance_max"] = np.max(cov_matrix)
#     features["covariance_min"] = np.min(cov_matrix)
#     features["covariance_std"] = np.std(cov_matrix)

#     return features


# # Step 2: æ‰¹é‡æå–å¹¶è½¬æ¢ä¸ºå¼ é‡çš„å‡½æ•°
# def batch_extract_meta_features(batch_x):

#     print(batch_x.shape)
#     try:
#         batch_x = batch_x.numpy()  # å¦‚æœæ˜¯ tensor å°±è½¬æˆ numpy
#     except AttributeError:
#         pass  # å·²ç»æ˜¯ numpyï¼Œæ— éœ€å¤„ç†

#     batch_meta_features = []
#     for i in range(len(batch_x)):
#         meta_features = extract_meta_feature(batch_x[i])
#         batch_meta_features.append(meta_features)

#     # è½¬ä¸º PyTorch å¼ é‡ (float32 ç±»å‹)
#     meta_tensor = torch.tensor(batch_meta_features, dtype=torch.float32)
#     # è½¬ä¸º DataFrame
#     batch_meta_features = pd.DataFrame(batch_meta_features)

#     return batch_meta_features, meta_tensor


# # Step 3: æµ‹è¯•ç”¨ä¾‹
# def test_batch_extract_meta_features():
#     print("ğŸš€ å¼€å§‹æµ‹è¯• batch_extract_meta_features å‡½æ•°...\n")

#     # åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥ï¼š(batch_size=4, sequence_length=100) çš„æ—¶é—´åºåˆ—æ•°æ®
#     np.random.seed(42)
#     fake_data = np.random.randn(4, 10, 100)  # 4 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ª 100 ä¸ªç‚¹

#     # åŒ…è£…æˆ PyTorch Tensorï¼ˆæ¨¡æ‹Ÿ DataLoader è¾“å‡ºï¼‰
#     batch_x = torch.tensor(fake_data, dtype=torch.float32)

#     print(f"è¾“å…¥æ•°æ®å½¢çŠ¶: {batch_x.shape} (batch_size, sequence_length)")

#     # è°ƒç”¨å‡½æ•°æå–å…ƒç‰¹å¾å¹¶è½¬ä¸ºå¼ é‡
#     batch_meta_features, meta_features_tensor = batch_extract_meta_features(batch_x)

#     print(f"è¾“å‡ºå¼ é‡å½¢çŠ¶: {meta_features_tensor.shape} (åº”ä¸º [4, 7] å› ä¸ºæœ‰ 7 ä¸ªå…ƒç‰¹å¾)")
#     print("è¾“å‡ºå¼ é‡å†…å®¹:")
#     print(meta_features_tensor)

#     # é¢å¤–ï¼šæ‰“å°åŸå§‹ DataFrame æŸ¥çœ‹ç»“æ„

#     print(batch_meta_features.round(4))

#     print("\nâœ… æµ‹è¯•é€šè¿‡ï¼")


# # Step 4: è¿è¡Œæµ‹è¯•
# if __name__ == "__main__":
#     test_batch_extract_meta_features()

# xxxxxxx æµ‹è¯•meta_feature_vè®¡ç®—å…ƒç‰¹å¾çš„æ–¹æ³•

# import torch
# import torch.nn as nn

# import time
# import torch
# from layers.meta_feature_v import (
#     batch_extract_meta_features_gpu,
#     extract_meta_features_per_variable_gpu,
#     batch_extract_meta_features,
# )


# def test_meta_batch():
#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#     B, T, N = 32, 512, 200
#     x = torch.randn(B, T, N, device=device)
#     seq_len, pred_len = T, 720
#     if device.type == "cuda":
#         torch.cuda.synchronize()
#     t0 = time.perf_counter()
#     m_batch = batch_extract_meta_features_gpu(x, seq_len, pred_len)
#     if device.type == "cuda":
#         torch.cuda.synchronize()
#     t1 = time.perf_counter()
#     #     feats = []
#     #     if device.type == "cuda":
#     #         torch.cuda.synchronize()
#     #     t2 = time.perf_counter()
#     #     for b in range(B):
#     #         feats.append(extract_meta_features_per_variable_gpu(x[b], seq_len, pred_len))
#     #     m_stack = torch.stack(feats, dim=0)
#     #     if device.type == "cuda":
#     #         torch.cuda.synchronize()
#     #     t3 = time.perf_counter()
#     #     eq = torch.allclose(m_batch, m_stack, atol=1e-6, rtol=1e-5)
#     #     diff = (m_batch - m_stack).abs().max().item()
#     print("shape_batch", m_batch.shape)
#     # print("shape_stack", m_stack.shape)
#     # print("equal", eq)
#     # print("max_abs_diff", diff)
#     print("time_batch_s", t1 - t0)
#     # print("time_stack_s", t3 - t2)
#     if device.type == "cuda":
#         torch.cuda.synchronize()
#     t2 = time.perf_counter()
#     meta_cpu = batch_extract_meta_features(x, seq_len, pred_len)
#     if device.type == "cuda":
#         torch.cuda.synchronize()
#     t3 = time.perf_counter()
#     print("shape_batch_cpu", meta_cpu.shape)
#     print("cpu_time_batch_s", t3 - t2)


# if __name__ == "__main__":
#     test_meta_batch()


# def fuse_ts2img_select_best(ts2img_tensor_list, weights):
#     """
#     å¯¹æ¯ä¸ªæ ·æœ¬ï¼Œé€‰æ‹©æƒé‡æœ€å¤§çš„é‚£ç§ TS2Img è¡¨ç¤ºè¿›è¡Œèåˆï¼ˆæ ·æœ¬çº§è‡ªé€‚åº”é€‰æ‹©ï¼‰ã€‚

#     Args:
#         ts2img_tensor_list (list of torch.Tensor): æ¯ä¸ªå…ƒç´ å½¢çŠ¶ä¸º (B, C, H, W)
#         weights (torch.Tensor): å½¢çŠ¶ä¸º (B, d_ts2img)ï¼Œè¡¨ç¤ºæ¯ä¸ªæ ·æœ¬åœ¨æ¯ç§ TS2Img æ–¹æ³•ä¸Šçš„æƒé‡

#     Returns:
#         fused_tensor (torch.Tensor): å½¢çŠ¶ä¸º (B, C, H, W)ï¼Œèåˆåçš„ç»“æœ
#     """
#     assert len(ts2img_tensor_list) > 0, "ts2img_tensor_list ä¸èƒ½ä¸ºç©º"
#     B, C, H, W = ts2img_tensor_list[0].shape
#     d_ts2img = len(ts2img_tensor_list)

#     # æ£€æŸ¥ shapes æ˜¯å¦ä¸€è‡´
#     for i, tensor in enumerate(ts2img_tensor_list):
#         assert tensor.shape == (
#             B,
#             C,
#             H,
#             W,
#         ), f"ç¬¬ {i} ä¸ª tensor å½¢çŠ¶ä¸åŒ¹é…: {tensor.shape}"

#     assert weights.shape == (
#         B,
#         d_ts2img,
#     ), f"æƒé‡å½¢çŠ¶åº”ä¸º (B, d_ts2img)ï¼Œä½†å¾—åˆ° {weights.shape}"

#     # è·å–æ¯ä¸ªæ ·æœ¬æƒé‡æœ€å¤§çš„æ–¹æ³•ç´¢å¼•: shape (B,)
#     best_indices = torch.argmax(weights, dim=-1)  # (B,)

#     # æ„å»ºè¾“å‡º: å¯¹æ¯ä¸ªæ ·æœ¬ iï¼Œå– ts2img_tensor_list[best_indices[i]][i]
#     fused_tensors = []
#     A = weights.shape[0]
#     print(A, B)
#     for i in range(B):
#         chosen_idx = best_indices[i].item()  # è½¬ä¸º Python int
#         selected_tensor = ts2img_tensor_list[chosen_idx][i]  # (C, H, W)
#         fused_tensors.append(selected_tensor)

#     # å †å å› batch ç»´åº¦
#     fused_tensor = torch.stack(fused_tensors)  # (B, C, H, W)
#     return fused_tensor


# # ========================================
# # ğŸ”§ æµ‹è¯•ä»£ç 
# # ========================================


# def test_fuse_ts2img_select_best():
#     print("å¼€å§‹æµ‹è¯• fuse_ts2img_select_best å‡½æ•°...\n")

#     # è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯å¤ç°
#     torch.manual_seed(42)

#     # æ¨¡æ‹Ÿå‚æ•°
#     B = 4  # batch size
#     C = 3  # channel
#     H = 32  # height
#     W = 32  # width
#     d_ts2img = 3  # ä¸‰ç§ TS2Img æ–¹æ³•ï¼šæ¯”å¦‚ GAF, MTF, RP

#     print(f"æ„é€ æ•°æ®ï¼šB={B}, C={C}, H={H}, W={W}, d_ts2img={d_ts2img}")

#     # æ„é€  ts2img_tensor_list: 3 ä¸ª (B, C, H, W) çš„å¼ é‡
#     ts2img_tensor_list = []
#     method_names = ["GAF", "MTF", "RP"]
#     for i in range(d_ts2img):
#         # æ¨¡æ‹Ÿä¸åŒæ–¹æ³•ç”Ÿæˆçš„å›¾åƒè¡¨ç¤ºï¼ˆéšæœºåˆå§‹åŒ–ï¼‰
#         tensor = torch.randn(B, C, H, W) + i * 0.5  # ç¨å¾®åç§»ä»¥ä¾¿åŒºåˆ†
#         ts2img_tensor_list.append(tensor)
#         print(f"{method_names[i]} tensor shape: {tensor.shape}")

#     # æ„é€  weights: (B, d_ts2img)ï¼Œè¡¨ç¤ºæ¯ä¸ªæ ·æœ¬å¯¹ä¸‰ç§æ–¹æ³•çš„åå¥½
#     weights = torch.softmax(torch.randn(B, d_ts2img), dim=-1)
#     print(f"\nWeights (softmax å): \n{weights}\n")

#     # æ‰“å°æ¯ä¸ªæ ·æœ¬é€‰æ‹©çš„æ˜¯å“ªä¸ªæ–¹æ³•
#     best_indices = torch.argmax(weights, dim=-1)
#     print("æ¯ä¸ªæ ·æœ¬é€‰æ‹©çš„æ–¹æ³•ç´¢å¼•:", best_indices.tolist())
#     print("å¯¹åº”æ–¹æ³•:", [method_names[i] for i in best_indices.tolist()])

#     # æ‰§è¡Œèåˆ
#     fused_tensor = fuse_ts2img_select_best(ts2img_tensor_list, weights)

#     print(f"\nèåˆåè¾“å‡ºå½¢çŠ¶: {fused_tensor.shape}")
#     assert fused_tensor.shape == (B, C, H, W), "è¾“å‡ºå½¢çŠ¶é”™è¯¯ï¼"

#     # éªŒè¯æŸä¸ªæ ·æœ¬æ˜¯å¦æ­£ç¡®é€‰æ‹©
#     for i in range(B):
#         chosen_idx = best_indices[i].item()
#         expected = ts2img_tensor_list[chosen_idx][i]
#         actual = fused_tensor[i]
#         assert torch.allclose(expected, actual), f"æ ·æœ¬ {i} é€‰æ‹©é”™è¯¯ï¼"
#     print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼èåˆé€»è¾‘æ­£ç¡®ã€‚")

#     return fused_tensor


# # è¿è¡Œæµ‹è¯•
# if __name__ == "__main__":
#     result = test_fuse_ts2img_select_best()

# xxxxxxx
# import torch
# import torch.nn as nn


# # åˆ¤æ–­ä¸€ä¸ªå½¢çŠ¶ä¸º [C, H, W] çš„ Tensor æ˜¯å¦ä¸ºç°åº¦å›¾åƒ
# def is_grayscale_tensor(tensor, tol=1e-6):

#     if tensor.shape[0] == 1:
#         return True
#     elif tensor.shape[0] == 3:
#         # æ‹†åˆ†ä¸‰ä¸ªé€šé“
#         r, g, b = tensor[0], tensor[1], tensor[2]

#         # æ£€æŸ¥ R å’Œ G çš„å·®å¼‚ï¼ŒR å’Œ B çš„å·®å¼‚
#         diff_rg = torch.abs(r - g)
#         diff_rb = torch.abs(r - b)

#         # å¦‚æœæ‰€æœ‰å·®å¼‚éƒ½å°äºå®¹å¿åº¦ï¼Œåˆ™è®¤ä¸ºæ˜¯ç°åº¦å›¾
#         return (diff_rg < tol).all() and (diff_rb < tol).all()
#     else:
#         raise ValueError("Warning: Unexpected number of channels")


# def fuse_ts2img_top3_grayscale_stack(ts2img_tensor_list, weights):
#     """
#     å¯¹æ¯ä¸ªæ ·æœ¬é€‰æ‹©æƒé‡æœ€é«˜çš„ Top-3 TS2Img è¡¨ç¤ºï¼Œ
#     è‹¥ä¸ºç°åº¦å›¾åˆ™å–å•é€šé“ï¼Œæœ€ååœ¨é€šé“ç»´åº¦å †å æˆæ–°çš„ (B, 3, H, W) è¡¨ç¤ºã€‚

#     Args:
#         ts2img_tensor_list (list of torch.Tensor): æ¯ä¸ªå½¢çŠ¶ä¸º (B, 3, H, W)
#         weights (torch.Tensor): å½¢çŠ¶ä¸º (B, d_ts2img)

#     Returns:
#         fused_tensor (torch.Tensor): (B, 3, H, W)ï¼Œç”± Top-3 çš„å•é€šé“å›¾åƒå †å è€Œæˆ
#     """
#     assert (
#         len(ts2img_tensor_list) >= 3
#     ), "ts2img_tensor_list è‡³å°‘è¦æœ‰ 3 ä¸ªè¡¨ç¤ºæ‰èƒ½é€‰ Top-3"
#     B, C, H, W = ts2img_tensor_list[0].shape
#     assert C == 3, "æ¯ä¸ª TS2Img è¡¨ç¤ºåº”ä¸º 3 é€šé“"
#     d_ts2img = len(ts2img_tensor_list)
#     assert weights.shape == (
#         B,
#         d_ts2img,
#     ), f"æƒé‡å½¢çŠ¶åº”ä¸º (B, {d_ts2img})ï¼Œä½†å¾—åˆ° {weights.shape}"

#     # è·å– Top-3 ç´¢å¼• (B, 3)
#     top3_indices = torch.topk(weights, k=3, dim=-1).indices  # (B, 3)

#     fused_batch = []
#     for i in range(B):
#         # å½“å‰æ ·æœ¬é€‰æ‹©çš„ä¸‰ç§æ–¹æ³•ç´¢å¼•
#         idx0, idx1, idx2 = top3_indices[i].tolist()

#         channels = []
#         for method_idx in [idx0, idx1, idx2]:
#             img_3ch = ts2img_tensor_list[method_idx][i]  # (3, H, W)

#             # åˆ¤æ–­æ˜¯å¦ä¸ºç°åº¦å›¾
#             if is_grayscale_tensor(img_3ch):
#                 # å–ç¬¬ä¸€ä¸ªé€šé“ä½œä¸ºç°åº¦å€¼
#                 gray_channel = img_3ch[0:1]  # (1, H, W)
#             else:
#                 # å¦‚æœä¸æ˜¯ç°åº¦å›¾ï¼Œä¹Ÿå–ç¬¬ä¸€ä¸ªé€šé“ï¼ˆæˆ–å¯æ”¹ä¸ºå¹³å‡ï¼‰
#                 gray_channel = img_3ch[0:1]  # (1, H, W)

#             channels.append(gray_channel)

#         # å°†ä¸‰ä¸ª (1, H, W) é€šé“å †å æˆ (3, H, W)
#         fused_img = torch.cat(channels, dim=0)  # (3, H, W)
#         fused_batch.append(fused_img)

#     # å †å æˆ batch
#     fused_tensor = torch.stack(fused_batch)  # (B, 3, H, W)
#     return fused_tensor


# # ========================================
# # ğŸ”§ æµ‹è¯•ä»£ç 
# # ========================================


# def test_fuse_top3_grayscale_stack():
#     print("å¼€å§‹æµ‹è¯• Top-3 ç°åº¦å›¾å †å èåˆç­–ç•¥...\n")
#     torch.manual_seed(42)

#     # å‚æ•°
#     B = 2
#     H, W = 16, 16
#     d_ts2img = 5  # æœ‰5ç§ TS2Img æ–¹æ³•

#     print(f"æ„é€ æ•°æ®ï¼šB={B}, H={H}, W={W}, d_ts2img={d_ts2img}")

#     # æ„é€  ts2img_tensor_list
#     ts2img_tensor_list = []
#     method_names = [f"Method_{i}" for i in range(d_ts2img)]

#     for i in range(d_ts2img):
#         if i % 2 == 0:
#             # å¶æ•°æ–¹æ³•ï¼šæ„é€ ç°åº¦å›¾ï¼ˆä¸‰é€šé“ç›¸åŒï¼‰
#             gray_value = torch.randn(1, H, W)
#             tensor = torch.cat([gray_value] * 3, dim=0)  # (3, H, W)
#             print(f"{method_names[i]}: ç°åº¦å›¾")
#         else:
#             # å¥‡æ•°æ–¹æ³•ï¼šæ„é€ å½©è‰²å›¾ï¼ˆä¸‰é€šé“ä¸åŒï¼‰
#             tensor = torch.randn(3, H, W)
#             print(f"{method_names[i]}: éç°åº¦å›¾")

#         # æ‰©å±•ä¸º batch
#         batch_tensor = tensor.unsqueeze(0).repeat(B, 1, 1, 1)  # (B, 3, H, W)
#         ts2img_tensor_list.append(batch_tensor)

#     # æ„é€ æƒé‡ (B, d_ts2img)
#     weights = torch.softmax(torch.randn(B, d_ts2img), dim=-1)
#     print(f"\nWeights:\n{weights}\n")

#     # è·å–æ¯ä¸ªæ ·æœ¬çš„ Top-3
#     top3_indices = torch.topk(weights, 3, dim=-1).indices
#     print("æ¯ä¸ªæ ·æœ¬é€‰æ‹©çš„ Top-3 æ–¹æ³•ç´¢å¼•:")
#     for i in range(B):
#         selected = [method_names[idx] for idx in top3_indices[i].tolist()]
#         print(f"  æ ·æœ¬ {i}: {selected}")

#     # æ‰§è¡Œèåˆ
#     fused_tensor = fuse_ts2img_top3_grayscale_stack(ts2img_tensor_list, weights)
#     print(f"\nèåˆåå½¢çŠ¶: {fused_tensor.shape}")
#     assert fused_tensor.shape == (B, 3, H, W), "è¾“å‡ºå½¢çŠ¶é”™è¯¯ï¼"

#     # éªŒè¯ï¼šæ£€æŸ¥æ˜¯å¦çœŸçš„ä» Top-3 ä¸­æå–å¹¶å †å 
#     for i in range(B):
#         idx0, idx1, idx2 = top3_indices[i].tolist()
#         ch0 = ts2img_tensor_list[idx0][i][0]  # å–ç¬¬ä¸€ä¸ªé€šé“
#         ch1 = ts2img_tensor_list[idx1][i][0]
#         ch2 = ts2img_tensor_list[idx2][i][0]

#         expected = torch.stack([ch0, ch1, ch2], dim=0)  # (3, H, W)
#         actual = fused_tensor[i]

#         assert torch.allclose(expected, actual), f"æ ·æœ¬ {i} èåˆç»“æœä¸åŒ¹é…ï¼"
#     print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Top-3 ç°åº¦å›¾å †å èåˆé€»è¾‘æ­£ç¡®ã€‚")

#     return fused_tensor


# # è¿è¡Œæµ‹è¯•
# if __name__ == "__main__":
#     result = test_fuse_top3_grayscale_stack()

# xxxxxxxxxxxxxxxxxxx

# import numpy as np
# import matplotlib.pyplot as plt
# from io import BytesIO
# from PIL import Image
# import torch
# from torchvision import transforms

# # ç”Ÿæˆæµ‹è¯•æ•°æ®ï¼ˆæ¨¡æ‹Ÿå°æ³¢å˜æ¢ç»“æœï¼‰
# np.random.seed(42)  # å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿å¯å¤ç°
# wavelet_data = np.random.rand(20, 100)  # 20ä¸ªå°ºåº¦ï¼Œ100ä¸ªæ—¶é—´ç‚¹
# single_series = np.arange(100)  # æ¨¡æ‹Ÿæ—¶é—´åºåˆ—
# scales = np.arange(20)  # æ¨¡æ‹Ÿå°ºåº¦
# W, H = 224, 224  # ç›®æ ‡å›¾åƒå°ºå¯¸


# def generate_tensor_and_save(color_map, save_path):
#     """æ ¹æ®æŒ‡å®šçš„colormapç”Ÿæˆå›¾åƒå¼ é‡å¹¶ä¿å­˜å›¾åƒåˆ°æœ¬åœ°"""
#     fig, ax = plt.subplots(figsize=(5, 3))  # å›ºå®šç”»å¸ƒå¤§å°

#     # ç»˜åˆ¶å°æ³¢å›¾
#     im = ax.imshow(
#         wavelet_data,
#         origin="upper",
#         aspect="auto",
#         extent=[0, len(single_series), 0, len(scales)],
#         cmap=color_map,
#     )
#     plt.axis("off")  # å…³é—­åæ ‡è½´ï¼Œé¿å…å¹²æ‰°

#     # ä¿å­˜åˆ°å†…å­˜ç¼“å†²åŒº
#     with BytesIO() as buf:
#         plt.savefig(buf, format="png", bbox_inches="tight", pad_inches=0)
#         buf.seek(0)

#         # è½¬æ¢ä¸ºç°åº¦å›¾
#         with Image.open(buf) as img:
#             img_gray = img.convert("L")  # è½¬ä¸ºå•é€šé“ç°åº¦å›¾
#             img_resized = img_gray.resize((W, H), Image.Resampling.LANCZOS)

#             # ä¿å­˜å›¾åƒåˆ°æœ¬åœ°
#             img_resized.save(save_path)

#             # è½¬æ¢ä¸ºå¼ é‡
#             transform = transforms.ToTensor()
#             tensor = transform(img_resized)

#     plt.close(fig)  # é‡Šæ”¾å†…å­˜
#     return tensor


# # ç”Ÿæˆå¹¶ä¿å­˜ä¸¤ç§å›¾åƒ
# tensor_rainbow = generate_tensor_and_save("rainbow", "rainbow_to_gray.png")
# tensor_gray = generate_tensor_and_save("gray", "gray_to_gray.png")

# # æ‰“å°å¼ é‡ä¿¡æ¯
# print(f"å½©è™¹è‰²æ˜ å°„è½¬ç°åº¦å¼ é‡å½¢çŠ¶: {tensor_rainbow.shape}")
# print(f"ç°åº¦æ˜ å°„è½¬ç°åº¦å¼ é‡å½¢çŠ¶: {tensor_gray.shape}")
# print(f"ä¸¤ä¸ªå¼ é‡æ˜¯å¦å®Œå…¨ç›¸åŒ: {torch.allclose(tensor_rainbow, tensor_gray, atol=1e-6)}")

# # è®¡ç®—å¼ é‡å·®å¼‚ï¼ˆç»Ÿè®¡æœ€å¤§å€¼ï¼‰
# diff = torch.abs(tensor_rainbow - tensor_gray)
# print(f"å¼ é‡å…ƒç´ æœ€å¤§å·®å¼‚: {diff.max().item():.6f}")


# xxxxxxxxxxxxxxxxxxxxxxxxx
# import torch
# import torch.nn.functional as F


# def adaptive_fusion(ts2img_tensor_list, ts2img_weights):
#     """
#     è‡ªé€‚åº”èåˆå¤šä¸ªæ—¶åºå›¾åƒåŒ–è¡¨ç¤ºã€‚

#     Args:
#         ts2img_tensor_list: list of tensors, each shape (B, C, H, W)
#         ts2img_weights: tensor of shape (B, d_ts2img), æƒé‡è¶Šå¤§è¶Šé‡è¦

#     Returns:
#         fused_tensor: (B, 3, H, W), æ¯ä¸ªæ ·æœ¬ç”±top3åŠ æƒè¡¨ç¤ºæ‹¼æ¥è€Œæˆ
#     """
#     B = ts2img_weights.size(0)
#     d_ts2img = len(ts2img_tensor_list)

#     assert d_ts2img == ts2img_weights.size(1), "æƒé‡ç»´åº¦åº”ä¸è¡¨ç¤ºæ•°é‡ä¸€è‡´"

#     # Step 1: å¤„ç†æ¯ä¸ªè¡¨ç¤ºï¼šè‹¥ C != 1ï¼Œåˆ™åœ¨é€šé“ç»´åº¦å–å¹³å‡ -> å˜æˆ (B, 1, H, W)
#     processed_tensors = []
#     for tensor in ts2img_tensor_list:
#         assert tensor.ndim == 4, f"æœŸæœ›4Då¼ é‡ (B,C,H,W)ï¼Œå¾—åˆ° {tensor.shape}"
#         if tensor.size(1) != 1:
#             # åœ¨é€šé“ç»´åº¦å–å¹³å‡ï¼Œå¹¶ä¿æŒç»´åº¦
#             print("yasuo")
#             squeezed = tensor.mean(dim=1, keepdim=True)  # (B, 1, H, W)
#         else:
#             squeezed = tensor  # å·²ç»æ˜¯ (B, 1, H, W)
#         processed_tensors.append(squeezed)

#     # Step 2: è·å–æ¯ä¸ªæ ·æœ¬ top-3 çš„ index (åŸºäºæƒé‡)
#     _, topk_indices = torch.topk(ts2img_weights, k=3, dim=1)  # (B, 3)

#     # Step 3: æ„é€ è¾“å‡ºå¼ é‡ (B, 3, H, W)
#     device = processed_tensors[0].device
#     dtype = processed_tensors[0].dtype
#     H, W = processed_tensors[0].shape[2], processed_tensors[0].shape[3]

#     fused_tensor = torch.zeros(B, 3, H, W, device=device, dtype=dtype)

#     for b in range(B):
#         for i in range(3):
#             modality_idx = topk_indices[b, i].item()
#             # å–å‡ºå¯¹åº”æ¨¡æ€çš„ (B, 1, H, W) ä¸­ç¬¬ b ä¸ªæ ·æœ¬
#             tempt = processed_tensors[modality_idx][b : b + 1, :, :, :]
#             # print(f"shape{tempt.shape}")
#             fused_tensor[b, i : i + 1, :, :] = tempt
#     # print(f"shape2{fused_tensor.shape}")
#     return fused_tensor


# def test_adaptive_fusion():
#     print("å¼€å§‹æµ‹è¯• adaptive_fusion å‡½æ•°...")

#     # è®¾ç½®éšæœºç§å­ä»¥ä¾¿å¤ç°
#     torch.manual_seed(42)

#     B = 2
#     H, W = 2, 2
#     d_ts2img = 5

#     # æ„é€ è¾“å…¥ï¼šä¸åŒ C å€¼çš„å¼ é‡åˆ—è¡¨
#     ts2img_tensor_list = [
#         torch.randn(B, 1, H, W),  # C=1ï¼Œæ— éœ€å¤„ç†
#         torch.randn(B, 1, H, W),  # C=4ï¼Œéœ€å¹³å‡
#         torch.randn(B, 1, H, W),  # C=1
#         torch.randn(B, 1, H, W),  # C=3ï¼Œéœ€å¹³å‡
#         torch.randn(B, 1, H, W),  # C=1
#     ]

#     print(ts2img_tensor_list)
#     # æƒé‡ï¼š(B, d_ts2img)
#     ts2img_weights = torch.randn(B, d_ts2img)
#     print(f"æƒé‡çŸ©é˜µ:\n{ts2img_weights}\n")

#     # æ‰§è¡Œèåˆ
#     fused_output = adaptive_fusion(ts2img_tensor_list, ts2img_weights)

#     print(fused_output)
#     # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
#     assert fused_output.shape == (B, 3, H, W), f"è¾“å‡ºå½¢çŠ¶é”™è¯¯: {fused_output.shape}"
#     print(f"âœ… è¾“å‡ºå½¢çŠ¶æ­£ç¡®: {fused_output.shape}")

#     # éªŒè¯æ¯ä¸ªæ ·æœ¬ç¡®å®æ˜¯æ¥è‡ª top-3 æƒé‡å¯¹åº”çš„æ¨¡æ€
#     _, topk_indices = torch.topk(ts2img_weights, k=3, dim=1)

#     print("\né€æ ·æœ¬éªŒè¯...")
#     for b in range(B):
#         print(f"\næ ·æœ¬ {b}:")
#         print(f"  Top-3 æ¨¡æ€ç´¢å¼•: {topk_indices[b].tolist()}")

#         for i in range(3):
#             mod_idx = topk_indices[b, i].item()
#             expected_slice = ts2img_tensor_list[mod_idx][b]
#             if expected_slice.size(0) != 1:  # å¦‚æœåŸå§‹ C > 1ï¼Œåº”å·²å¹³å‡
#                 expected_slice = expected_slice.mean(dim=0, keepdim=True)  # (1, H, W)

#             actual_slice = fused_output[b, i]  # (H, W)

#             # æ£€æŸ¥æ˜¯å¦ç›¸ç­‰
#             diff = (actual_slice - expected_slice.squeeze()).abs().max()
#             assert diff < 1e-6, f"æ ·æœ¬{b}, ä½ç½®{i}: ä¸åŒ¹é…, æœ€å¤§è¯¯å·®={diff}"
#             print(f"    ä½ç½® {i}: æ¥è‡ªæ¨¡æ€ {mod_idx}, åŒ¹é… âœ“")

#     print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")


# # è¿è¡Œæµ‹è¯•
# if __name__ == "__main__":
#     test_adaptive_fusion()

# xxxxxxxxxxxxxxxxxxxx
# import torch


# def weighted_sum_fusion(ts2img_tensor_list, ts2img_weights):
#     """
#     å¯¹å¤šä¸ªæ—¶åºå›¾åƒåŒ–è¡¨ç¤ºè¿›è¡ŒåŠ æƒåŠ å’Œèåˆã€‚

#     Args:
#         ts2img_tensor_list (list of torch.Tensor): æ¯ä¸ªå¼ é‡å½¢çŠ¶ä¸º (B, C, H, W)
#         ts2img_weights (torch.Tensor): å½¢çŠ¶ä¸º (B, d_ts2img)

#     Returns:
#         fused_tensor (torch.Tensor): å½¢çŠ¶ä¸º (B, C, H, W)ï¼ŒåŠ æƒèåˆç»“æœ
#     """
#     if len(ts2img_tensor_list) == 0:
#         raise ValueError("ts2img_tensor_list ä¸èƒ½ä¸ºç©º")

#     B, d_ts2img = ts2img_weights.shape
#     assert (
#         len(ts2img_tensor_list) == d_ts2img
#     ), f"è¡¨ç¤ºæ•°é‡({len(ts2img_tensor_list)})åº”ä¸æƒé‡ç¬¬äºŒç»´({d_ts2img})ä¸€è‡´"

#     # æ£€æŸ¥æ‰€æœ‰å¼ é‡å½¢çŠ¶æ˜¯å¦ä¸€è‡´
#     ref_shape = ts2img_tensor_list[0].shape
#     C, H, W = ref_shape[1], ref_shape[2], ref_shape[3]
#     for i, tensor in enumerate(ts2img_tensor_list):
#         if tensor.shape != ref_shape:
#             raise ValueError(
#                 f"å¼ é‡ {i} å½¢çŠ¶ {tensor.shape} ä¸å‚è€ƒå½¢çŠ¶ {ref_shape} ä¸ä¸€è‡´"
#             )

#     # å°†åˆ—è¡¨å †å æˆ (B, d_ts2img, C, H, W)
#     stacked = torch.stack(ts2img_tensor_list, dim=1)  # â†’ (B, d_ts2img, C, H, W)

#     print(f"stacked.shape{stacked.shape}")
#     # æ‰©å±•æƒé‡åˆ° (B, d_ts2img, 1, 1, 1)ï¼Œä»¥ä¾¿å¹¿æ’­
#     weights_expanded = (
#         ts2img_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)
#     )  # â†’ (B, d_ts2img, 1, 1, 1)

#     print(f"weights_expanded.shape{weights_expanded.shape}")
#     # åŠ æƒæ±‚å’Œ: (B, d_ts2img, C, H, W) * (B, d_ts2img, 1, 1, 1) â†’ (B, C, H, W)
#     fused_tensor = torch.sum(stacked * weights_expanded, dim=1)

#     print(f"fused_tensor.shape{fused_tensor.shape}")
#     return fused_tensor


# def test_weighted_sum_fusion():
#     print("å¼€å§‹æµ‹è¯• weighted_sum_fusion å‡½æ•°...\n")
#     torch.manual_seed(42)

#     # è®¾ç½®å‚æ•°
#     B = 2
#     C = 3
#     H, W = 2, 2
#     d_ts2img = 4

#     # æ„é€ è¾“å…¥ï¼š4 ç§ä¸åŒçš„æ—¶åºå›¾åƒåŒ–è¡¨ç¤º
#     ts2img_tensor_list = [
#         torch.randn(B, C, H, W) * 1.0 + 0.0,
#         torch.randn(B, C, H, W) * 1.0 + 1.0,
#         torch.randn(B, C, H, W) * 1.0 + 2.0,
#         torch.randn(B, C, H, W) * 1.0 + 3.0,
#     ]

#     # æ„é€ æƒé‡ï¼š(B, d_ts2img)ï¼Œæ¯è¡Œå’Œä¸ä¸€å®šä¸º1ï¼ˆæ”¯æŒä»»æ„æƒé‡ï¼‰
#     ts2img_weights = torch.softmax(
#         torch.randn(B, d_ts2img), dim=1
#     )  # ä½¿ç”¨ softmax å½’ä¸€åŒ–
#     print(f"æƒé‡çŸ©é˜µ (softmax å½’ä¸€åŒ–):\n{ts2img_weights}\n")

#     # æ‰§è¡Œèåˆ
#     fused_output = weighted_sum_fusion(ts2img_tensor_list, ts2img_weights)

#     # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
#     assert fused_output.shape == (B, C, H, W), f"è¾“å‡ºå½¢çŠ¶é”™è¯¯: {fused_output.shape}"
#     print(f"âœ… è¾“å‡ºå½¢çŠ¶æ­£ç¡®: {fused_output.shape}")

#     # é€æ ·æœ¬éªŒè¯
#     print("\né€æ ·æœ¬éªŒè¯...")
#     for b in range(B):
#         print(f"\n--- æ ·æœ¬ {b} ---")
#         expected = torch.zeros(C, H, W)  # æ‰‹åŠ¨è®¡ç®—åŠ æƒå’Œ
#         for i in range(d_ts2img):
#             weight = ts2img_weights[b, i].item()
#             rep = ts2img_tensor_list[i][b]  # (C, H, W)
#             expected += weight * rep
#             print(f"  æ¨¡æ€ {i}: æƒé‡={weight:.3f}")

#         actual = fused_output[b]
#         diff = (actual - expected).abs().max()
#         assert diff < 1e-6, f"æ ·æœ¬ {b} éªŒè¯å¤±è´¥ï¼Œæœ€å¤§è¯¯å·®={diff}"
#         print(f"âœ… æ ·æœ¬ {b} éªŒè¯é€šè¿‡ï¼Œæœ€å¤§è¯¯å·®: {diff:.2e}")

#     print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼åŠ æƒåŠ å’ŒèåˆåŠŸèƒ½æ­£å¸¸ã€‚")


# # è¿è¡Œæµ‹è¯•
# if __name__ == "__main__":
#     test_weighted_sum_fusion()

# from data_provider.data_factory import data_provider
# from exp.exp_basic import Exp_Basic
# from utils.tools import EarlyStopping, adjust_learning_rate, visual
# from utils.metrics import metric
# import torch
# import torch.nn as nn
# from torch import optim
# import os
# import time
# import warnings
# import numpy as np
# from utils.dtw_metric import dtw, accelerated_dtw
# from utils.augmentation import run_augmentation, run_augmentation_single

# warnings.filterwarnings("ignore")


# class Exp_Long_Term_Forecast(Exp_Basic):
#     def __init__(self, args):
#         super(Exp_Long_Term_Forecast, self).__init__(args)

#     def _build_model(self):
#         model = self.model_dict[self.args.model].Model(self.args).float()

#         if self.args.use_multi_gpu and self.args.use_gpu:
#             model = nn.DataParallel(model, device_ids=self.args.device_ids)
#         return model

#     def _get_data(self, flag):
#         data_set, data_loader = data_provider(self.args, flag)
#         return data_set, data_loader

#     def _select_optimizer(self):
#         model_optim = optim.Adam(self.model.parameters(), lr=self.args.learning_rate)
#         return model_optim

#     def _select_criterion(self):
#         criterion = nn.MSELoss()
#         return criterion

#     def vali(self, vali_data, vali_loader, criterion):
#         total_loss = []
#         self.model.eval()
#         with torch.no_grad():
#             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(
#                 vali_loader
#             ):
#                 batch_x = batch_x.float().to(self.device)
#                 batch_y = batch_y.float()

#                 batch_x_mark = batch_x_mark.float().to(self.device)
#                 batch_y_mark = batch_y_mark.float().to(self.device)

#                 # decoder input
#                 dec_inp = torch.zeros_like(batch_y[:, -self.args.pred_len :, :]).float()
#                 dec_inp = (
#                     torch.cat([batch_y[:, : self.args.label_len, :], dec_inp], dim=1)
#                     .float()
#                     .to(self.device)
#                 )
#                 # encoder - decoder
#                 if self.args.use_amp:
#                     with torch.cuda.amp.autocast():
#                         outputs = self.model(
#                             batch_x, batch_x_mark, dec_inp, batch_y_mark
#                         )
#                 else:
#                     outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
#                 f_dim = -1 if self.args.features == "MS" else 0
#                 outputs = outputs[:, -self.args.pred_len :, f_dim:]
#                 batch_y = batch_y[:, -self.args.pred_len :, f_dim:].to(self.device)

#                 pred = outputs.detach().cpu()
#                 true = batch_y.detach().cpu()

#                 loss = criterion(pred, true)

#                 total_loss.append(loss)
#         total_loss = np.average(total_loss)
#         self.model.train()
#         return total_loss

#     def train(self, setting):
#         train_data, train_loader = self._get_data(flag="train")
#         vali_data, vali_loader = self._get_data(flag="val")
#         test_data, test_loader = self._get_data(flag="test")

#         path = os.path.join(self.args.checkpoints, setting)
#         if not os.path.exists(path):
#             os.makedirs(path)

#         time_now = time.time()

#         train_steps = len(train_loader)
#         early_stopping = EarlyStopping(patience=self.args.patience, verbose=True)

#         model_optim = self._select_optimizer()
#         criterion = self._select_criterion()

#         if self.args.use_amp:
#             scaler = torch.cuda.amp.GradScaler()

#         for epoch in range(self.args.train_epochs):
#             iter_count = 0
#             train_loss = []

#             self.model.train()
#             epoch_time = time.time()
#             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(
#                 train_loader
#             ):
#                 iter_count += 1
#                 model_optim.zero_grad()
#                 batch_x = batch_x.float().to(self.device)
#                 batch_y = batch_y.float().to(self.device)
#                 batch_x_mark = batch_x_mark.float().to(self.device)
#                 batch_y_mark = batch_y_mark.float().to(self.device)

#                 # decoder input
#                 dec_inp = torch.zeros_like(batch_y[:, -self.args.pred_len :, :]).float()
#                 dec_inp = (
#                     torch.cat([batch_y[:, : self.args.label_len, :], dec_inp], dim=1)
#                     .float()
#                     .to(self.device)
#                 )

#                 # encoder - decoder
#                 if self.args.use_amp:
#                     with torch.cuda.amp.autocast():
#                         outputs = self.model(
#                             batch_x, batch_x_mark, dec_inp, batch_y_mark
#                         )

#                         f_dim = -1 if self.args.features == "MS" else 0
#                         outputs = outputs[:, -self.args.pred_len :, f_dim:]
#                         batch_y = batch_y[:, -self.args.pred_len :, f_dim:].to(
#                             self.device
#                         )
#                         loss = criterion(outputs, batch_y)
#                         train_loss.append(loss.item())
#                 else:
#                     outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)

#                     f_dim = -1 if self.args.features == "MS" else 0
#                     outputs = outputs[:, -self.args.pred_len :, f_dim:]
#                     batch_y = batch_y[:, -self.args.pred_len :, f_dim:].to(self.device)
#                     loss = criterion(outputs, batch_y)
#                     train_loss.append(loss.item())

#                 if (i + 1) % 100 == 0:
#                     print(
#                         "\titers: {0}, epoch: {1} | loss: {2:.7f}".format(
#                             i + 1, epoch + 1, loss.item()
#                         )
#                     )
#                     speed = (time.time() - time_now) / iter_count
#                     left_time = speed * (
#                         (self.args.train_epochs - epoch) * train_steps - i
#                     )
#                     print(
#                         "\tspeed: {:.4f}s/iter; left time: {:.4f}s".format(
#                             speed, left_time
#                         )
#                     )
#                     iter_count = 0
#                     time_now = time.time()

#                 if self.args.use_amp:
#                     scaler.scale(loss).backward()
#                     scaler.step(model_optim)
#                     scaler.update()
#                 else:
#                     loss.backward()
#                     model_optim.step()

#             print("Epoch: {} cost time: {}".format(epoch + 1, time.time() - epoch_time))
#             train_loss = np.average(train_loss)
#             vali_loss = self.vali(vali_data, vali_loader, criterion)
#             test_loss = self.vali(test_data, test_loader, criterion)

#             print(
#                 "Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}".format(
#                     epoch + 1, train_steps, train_loss, vali_loss, test_loss
#                 )
#             )
#             early_stopping(vali_loss, self.model, path)
#             if early_stopping.early_stop:
#                 print("Early stopping")
#                 break

#             adjust_learning_rate(model_optim, epoch + 1, self.args)

#         best_model_path = path + "/" + "checkpoint.pth"
#         self.model.load_state_dict(torch.load(best_model_path))

#         return self.model

#     def test(self, setting, test=0):
#         test_data, test_loader = self._get_data(flag="test")
#         if test:
#             print("loading model")
#             self.model.load_state_dict(
#                 torch.load(
#                     os.path.join("./checkpoints/" + setting, "checkpoint.pth"),
#                     map_location=self.device,
#                 )
#             )

#         preds = []
#         trues = []
#         folder_path = "./test_results/" + setting + "/"
#         if not os.path.exists(folder_path):
#             os.makedirs(folder_path)

#         self.model.eval()
#         with torch.no_grad():
#             for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(
#                 test_loader
#             ):
#                 batch_x = batch_x.float().to(self.device)
#                 batch_y = batch_y.float().to(self.device)

#                 batch_x_mark = batch_x_mark.float().to(self.device)
#                 batch_y_mark = batch_y_mark.float().to(self.device)

#                 # decoder input
#                 dec_inp = torch.zeros_like(batch_y[:, -self.args.pred_len :, :]).float()
#                 dec_inp = (
#                     torch.cat([batch_y[:, : self.args.label_len, :], dec_inp], dim=1)
#                     .float()
#                     .to(self.device)
#                 )
#                 # encoder - decoder
#                 if self.args.use_amp:
#                     with torch.cuda.amp.autocast():
#                         outputs = self.model(
#                             batch_x, batch_x_mark, dec_inp, batch_y_mark
#                         )
#                 else:
#                     outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)

#                 f_dim = -1 if self.args.features == "MS" else 0
#                 outputs = outputs[:, -self.args.pred_len :, :]
#                 batch_y = batch_y[:, -self.args.pred_len :, :].to(self.device)
#                 outputs = outputs.detach().cpu().numpy()
#                 batch_y = batch_y.detach().cpu().numpy()
#                 if test_data.scale and self.args.inverse:
#                     shape = outputs.shape
#                     outputs = test_data.inverse_transform(
#                         outputs.reshape(shape[0] * shape[1], -1)
#                     ).reshape(shape)
#                     batch_y = test_data.inverse_transform(
#                         batch_y.reshape(shape[0] * shape[1], -1)
#                     ).reshape(shape)

#                 outputs = outputs[:, :, f_dim:]
#                 batch_y = batch_y[:, :, f_dim:]

#                 pred = outputs
#                 true = batch_y

#                 preds.append(pred)
#                 trues.append(true)
#                 if i % 20 == 0:
#                     input = batch_x.detach().cpu().numpy()
#                     if test_data.scale and self.args.inverse:
#                         shape = input.shape
#                         input = test_data.inverse_transform(
#                             input.reshape(shape[0] * shape[1], -1)
#                         ).reshape(shape)
#                     gt = np.concatenate((input[0, :, -1], true[0, :, -1]), axis=0)
#                     pd = np.concatenate((input[0, :, -1], pred[0, :, -1]), axis=0)
#                     visual(gt, pd, os.path.join(folder_path, str(i) + ".pdf"))

#         preds = np.concatenate(preds, axis=0)
#         trues = np.concatenate(trues, axis=0)
#         print("test shape:", preds.shape, trues.shape)
#         preds = preds.reshape(-1, preds.shape[-2], preds.shape[-1])
#         trues = trues.reshape(-1, trues.shape[-2], trues.shape[-1])
#         print("test shape:", preds.shape, trues.shape)

#         # result save
#         folder_path = "./results/" + setting + "/"
#         if not os.path.exists(folder_path):
#             os.makedirs(folder_path)

#         # dtw calculation
#         if self.args.use_dtw:
#             dtw_list = []
#             manhattan_distance = lambda x, y: np.abs(x - y)
#             for i in range(preds.shape[0]):
#                 x = preds[i].reshape(-1, 1)
#                 y = trues[i].reshape(-1, 1)
#                 if i % 100 == 0:
#                     print("calculating dtw iter:", i)
#                 d, _, _, _ = accelerated_dtw(x, y, dist=manhattan_distance)
#                 dtw_list.append(d)
#             dtw = np.array(dtw_list).mean()
#         else:
#             dtw = "not calculated"

#         mae, mse, rmse, mape, mspe = metric(preds, trues)
#         print("mse: {}, mae: {}, dtw: {}".format(mse, mae, dtw))
#         f = open("result_long_term_forecast.txt", "a")
#         f.write(setting + "  \n")
#         f.write("mse: {}, mae: {}, dtw: {}".format(mse, mae, dtw))
#         f.write("\n")
#         f.write("\n")
#         f.close()

#         np.save(folder_path + "metrics.npy", np.array([mae, mse, rmse, mape, mspe]))
#         np.save(folder_path + "pred.npy", preds)
#         np.save(folder_path + "true.npy", trues)

#         return
# xxxxxxxxxxxxxxxxxxxxx
